{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import re\n",
    "\n",
    "\n",
    "def process(s):\n",
    "    mystem = Mystem()\n",
    "    pattern = r'[^0-9a-zа-яё\\s]'\n",
    "    s = s.lower()\n",
    "    s = re.sub(pattern, '', s)\n",
    "    s = re.sub(r'[\\s\\t]+', ' ', s)\n",
    "    lemmas = mystem.lemmatize(s)\n",
    "    return [i for i in lemmas[:-1] if i is not ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000 objects are processed...\n",
      "That took 9214.13 seconds.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, Lock, Value\n",
    "import time\n",
    "\n",
    "\n",
    "mutex = Lock()\n",
    "n_processed = Value('i', 0)\n",
    "\n",
    "with open('project/docs_titles.tsv') as f:\n",
    "    next(f)\n",
    "    def multiprocessing_func(line):\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = process(data[1])\n",
    "        with mutex:\n",
    "            global n_processed\n",
    "            n_processed.value += 1\n",
    "            if n_processed.value % 100 == 0:\n",
    "                print(f\"\\r{n_processed.value} objects are processed...\", end='', flush=True)\n",
    "        return (doc_id, title)\n",
    "    \n",
    "    with Pool() as pool:\n",
    "        starttime = time.time()\n",
    "        docs = pool.map(multiprocessing_func, [line for line in f])\n",
    "        print('\\nThat took {} seconds.'.format(round((time.time() - starttime), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took 0.01 seconds.\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "starttime = time.time()\n",
    "for doc in docs:\n",
    "    doc_to_title[doc[0]] = doc[1]\n",
    "print('That took {} seconds.'.format(round((time.time() - starttime), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('refinedDocTitles.tsv', 'w') as f:\n",
    "    for i in doc_to_title:\n",
    "        s = '{}\\t{}\\n'.format(i, doc_to_title[i])\n",
    "        f.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Preprocessing</h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('/home/oleg/sphere/dm/hw/project/train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 15) (11690,) (11690,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "for new_group in traingroups_titledata:\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title)\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = set(title_j)\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:15]    )\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "def batch_generator(X, y, shuffle=True, batch_size=1):\n",
    "    X_batch, y_batch = X, y\n",
    "    if shuffle is True:\n",
    "        from sklearn.utils import shuffle\n",
    "        X_batch, y_batch = shuffle(X, y)\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield X_batch[i:i + batch_size], y_batch[i:i + batch_size]\n",
    "\n",
    "def sigmoid(x):\n",
    "    sigm_value_x = 1 / (1 + np.exp(-x))\n",
    "    return sigm_value_x\n",
    "\n",
    "\n",
    "class MySGDClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, batch_generator, C=1, alpha=0.01,\n",
    "                 max_epoch=10, model_type='lin_reg', threshold=0.5):\n",
    "        self.C = C\n",
    "        self.alpha = alpha\n",
    "        self.max_epoch = max_epoch\n",
    "        self.batch_generator = batch_generator\n",
    "        self.errors_log = {'iter' : [], 'loss' : []}  \n",
    "        self.model_type = model_type\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def calc_loss(self, X_batch, y_batch):\n",
    "        N = X_batch.shape[0]\n",
    "        new_weights = self.weights.copy()\n",
    "        new_weights[0, 0] = 0\n",
    "        reg = 1 / self.C * new_weights.T.dot(new_weights)\n",
    "        if self.model_type == \"lin_reg\":\n",
    "            a = X_batch.dot(self.weights)\n",
    "            loss = 1 / N * (y_batch - a).T.dot(y_batch - a) + reg\n",
    "        elif self.model_type == \"log_reg\":\n",
    "            a = sigmoid(X_batch.dot(self.weights))\n",
    "            loss = -1 / N * (y_batch.T.dot(np.log(a)) + \n",
    "                             (1 - y_batch).T.dot(np.log(1 - a))) + reg\n",
    "        return loss\n",
    "    \n",
    "    def calc_loss_grad(self, X_batch, y_batch):\n",
    "        N = X_batch.shape[0]\n",
    "        new_weights = self.weights.copy()\n",
    "        new_weights[0, 0] = 0\n",
    "        reg = 2 / self.C * new_weights\n",
    "        if self.model_type == \"lin_reg\":\n",
    "            a = X_batch.dot(self.weights)\n",
    "            loss_grad = 2 / N * (X_batch.T).dot(a - y_batch) + reg\n",
    "        elif self.model_type == \"log_reg\":\n",
    "            a = sigmoid(X_batch.dot(self.weights))\n",
    "            loss_grad = 1 / N * X_batch.T.dot(a - y_batch) + reg\n",
    "        return loss_grad\n",
    "    \n",
    "    def update_weights(self, new_grad):\n",
    "        self.weights = self.weights - self.alpha * new_grad\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        y = y.reshape(y.shape[0], -1)\n",
    "        X_ones = np.ones((X.shape[0], 1))\n",
    "        X = np.hstack((X_ones, X))\n",
    "        self.weights = np.random.rand(X.shape[1]).reshape(X.shape[1], -1)\n",
    "        for n in range(0, self.max_epoch):\n",
    "            new_epoch_generator = self.batch_generator(X, y)\n",
    "            for batch_num, new_batch in enumerate(new_epoch_generator):\n",
    "                X_batch = new_batch[0]\n",
    "                y_batch = new_batch[1]                \n",
    "                batch_grad = self.calc_loss_grad(X_batch, y_batch)\n",
    "                self.update_weights(batch_grad)\n",
    "                batch_loss = self.calc_loss(X_batch, y_batch)\n",
    "                self.errors_log['iter'].append(batch_num)\n",
    "                self.errors_log['loss'].append(batch_loss)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_ones = np.ones((X.shape[0], 1))\n",
    "        X = np.hstack((X_ones, X))\n",
    "        if self.model_type==\"lin_reg\":\n",
    "            y_hat = X.dot(self.weights) > self.threshold\n",
    "        elif self.model_type==\"log_reg\":\n",
    "            y_hat = sigmoid(X.dot(self.weights)) > 0.5\n",
    "        return y_hat\n",
    "\n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        return np.mean(pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took 1305.01 seconds.\n",
      "{'C': 14, 'alpha': 0.01, 'max_epoch': 5, 'model_type': 'lin_reg', 'threshold': 0.30000000000000004} 0.6772595884306559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_param = {\n",
    "    'threshold': np.arange(0.1, 1, 0.1),\n",
    "    'C': np.arange(10, 20),\n",
    "    'alpha': [0.01],\n",
    "    'max_epoch': np.arange(1, 6),\n",
    "    'model_type': ['lin_reg'],\n",
    "}\n",
    "myclf = MySGDClassifier(batch_generator)\n",
    "gd_sr = GridSearchCV(estimator=myclf,\n",
    "                     param_grid=grid_param,\n",
    "                     scoring='f1',\n",
    "                     cv=5,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "\n",
    "starttime = time.time()\n",
    "gd_sr.fit(X_train, y_train)\n",
    "print('That took {} seconds.'.format(round((time.time() - starttime), 2)))\n",
    "\n",
    "best_parameters = gd_sr.best_params_\n",
    "best_score = gd_sr.best_score_\n",
    "print(best_parameters, best_score)\n",
    "best_parameters['batch_generator'] = batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 15) (16627,)\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('/home/oleg/sphere/dm/hw/project/test_groups.csv')\n",
    "testgroups_titledata = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in testgroups_titledata:\n",
    "        testgroups_titledata[doc_group] = []\n",
    "    testgroups_titledata[doc_group].append((doc_id, title))\n",
    "\n",
    "X_test = []\n",
    "groups_test = []\n",
    "for new_group in testgroups_titledata:\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "\n",
    "        groups_test.append(new_group)\n",
    "        all_dist = []\n",
    "        words = set(title)\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j = docs[j]\n",
    "            words_j = set(title_j)\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_test.append(sorted(all_dist, reverse=True)[0:15])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "groups_test = np.array(groups_test)\n",
    "X_test = StandardScaler().fit_transform(X_test)\n",
    "print(X_test.shape, groups_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclf = MySGDClassifier(**best_parameters)\n",
    "myclf.fit(X_train, y_train)\n",
    "y_pred = myclf.predict(X_test).astype(int).flatten()\n",
    "submission = pd.DataFrame({'pair_id': test_data['pair_id'], 'target': y_pred})\n",
    "submission.to_csv(\"submission4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 14,\n",
       " 'alpha': 0.01,\n",
       " 'max_epoch': 5,\n",
       " 'model_type': 'lin_reg',\n",
       " 'threshold': 0.30000000000000004,\n",
       " 'batch_generator': <function __main__.batch_generator(X, y, shuffle=True, batch_size=1)>}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
